# -*- coding: utf-8 -*-
"""Simple CNN model for classification MNIST dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XSPSQ_7Loo_4URj9-p_PqYfRVIQUGDBs
"""

# keras modules
import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Activation, Dense, Dropout
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten
from tensorflow.keras.utils import to_categorical, plot_model
from tensorflow.keras.datasets import mnist

"""# load mnist data"""

(x_train, y_train), (x_test, y_test) = mnist.load_data()

print('shape of x_train' , x_train.shape)
print('shape of y_train' , y_train.shape)
print('shape of x_test' , x_test.shape)
print('shape of y_test' , y_test.shape)

"""## Show a data item"""

import matplotlib.pyplot as plt
digit=x_train[3]
plt.imshow(digit, cmap=plt.cm.binary)
plt.show()

"""## Data preprocessing"""

# compute the number of labels
number_labels=len(np.unique(y_train))
number_labels

# convert y_train & y_test to one-hot vector
y_train_2=to_categorical(y_train)
y_test_2=to_categorical(y_test)

print('y_train before : ' ,y_train.shape )
print('y_train after : ' , y_train_2.shape)

"""## Adjust the dimensions to fit the network."""

print('x_train.shape :' , x_train.shape)
image_size=x_train.shape[1]
print('image size : ', image_size)

"""### resizeing & normalization"""

x_train = np.reshape(x_train,[-1, image_size, image_size, 1])
x_test = np.reshape(x_test,[-1, image_size, image_size, 1])
x_train = x_train.astype('float32') / 255
x_test = x_test.astype('float32') / 255

print('x_train after :',x_train.shape)
print('x_test after :' , x_test.shape)

"""## network parameters"""

inputs_shape=(image_size,image_size ,1)
batch_size=128
kernel_size=3
pool_size=2
filters=64
dropout=0.2

"""## The Model"""

import keras
from keras import layers

model=Sequential([
                  layers.Conv2D(filters=filters,
                                kernel_size=kernel_size,
                                activation='relu',
                                input_shape=inputs_shape,name='layer_one'),
                  layers.MaxPooling2D(pool_size,name='layer_max_one'),
                  layers.Conv2D(filters=filters,
                                kernel_size=kernel_size,
                                activation='relu',name='layer_2'),
                  layers.MaxPooling2D(pool_size,name='layer_max_2'),
                  layers.Conv2D(filters=filters,
                                kernel_size=kernel_size,
                                activation='relu', name='layer_3'),
                  layers.Flatten(name='layer_4'),
                  layers.Dropout(dropout,name='layer_5'),
                  layers.Dense(number_labels,name='layer_6'),
                  layers.Activation('softmax',name='layer_7'),

])

model.summary()

plot_model(model,to_file='model.png' ,show_shapes=True,show_dtype=True
           ,show_layer_activations=True)

# loss function for one-hot vector
# use of adam optimizer
# accuracy is good metric for classification tasks
model.compile(loss='categorical_crossentropy',
              optimizer='adam',
              metrics=['accuracy'])

# train the network
history=model.fit(x_train, y_train_2, epochs=10, batch_size=batch_size)

_, acc = model.evaluate(x_test,
                        y_test_2,
                        batch_size=batch_size,
                        verbose=0)
print("\nTest accuracy: %.1f%%" % (100.0 * acc))

# Print the results on the chart
# Accurasy
plt.plot(history.history['accuracy'])
plt.title('Model Accuracy vs Epoch')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')
plt.show()

# tracing the path of loss
plt.plot(history.history['loss'])
plt.title('Model Loss vs Epoch')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

"""### I will stop now, and I will continue later using new methods."""